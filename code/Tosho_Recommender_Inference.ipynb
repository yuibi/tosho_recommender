{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tosho Recommender (Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fbcf8fe8c50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gensim\n",
    "import janome\n",
    "from janome.tokenizer import Tokenizer as ja_tokenizer\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from model import *\n",
    "    \n",
    "def load_obj(filename):\n",
    "    with open(filename, 'rb') as handler:\n",
    "        return pickle.load(handler)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = load_obj('model/tosho_recommender_word_tokenizer.pkl')\n",
    "\n",
    "# Load Book2Vec (book-level representation)\n",
    "b2v = gensim.models.keyedvectors.KeyedVectors.load_word2vec_format('model/book2vec', binary=False)\n",
    "\n",
    "# Load book descriptions and titles\n",
    "text_pd = pd.read_csv('data/tosho_processed_clean.csv.bz2', sep='\\t', compression='bz2')\n",
    "\n",
    "# Load tf.keras model\n",
    "model = gru_model(\n",
    "                     embedding_dim=300,\n",
    "                     dropout_rate=0.209,\n",
    "                     rnn_unit=194,\n",
    "                     input_shape=(500,),\n",
    "                     num_features=20000+1,\n",
    "                     share_gru_weights_on_book=True,\n",
    "                     use_attention_on_book=True,\n",
    "                     use_attention_on_user=True,\n",
    "                     use_batch_norm=False,\n",
    "                     is_embedding_trainable=False,\n",
    "                     final_activation='tanh',\n",
    "                     final_dimension=392,\n",
    "                     embedding_matrix=np.zeros((20001, 300)))\n",
    "\n",
    "model.compile(loss='cosine_similarity',\n",
    "                  optimizer=Adam(lr=0.0044))\n",
    "\n",
    "x = np.ones((1, 500))\n",
    "y = np.ones((1, 392))\n",
    "\n",
    "model.train_on_batch([x, x, x, x], y)\n",
    "\n",
    "# Load the state of the old model\n",
    "model.load_weights('model/tosho_recommender_7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text:str):\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'\\s{1,}', '', text)\n",
    "    \n",
    "    text = re.sub(r'内容紹介', '', text)\n",
    "    text = re.sub(r'出版社からのコメント', '', text)\n",
    "    text = re.sub(r'商品の説明をすべて表示する', '', text)\n",
    "    text = re.sub(r'内容（「MARC」データベースより）', '', text)\n",
    "    text = re.sub(r'内容（「BOOK」データベースより）', '', text)\n",
    "\n",
    "    non_japanese = re.compile(r\"[^0-9\\-ぁ-ヶ亜-黑ー]\")\n",
    "    text = re.sub(non_japanese, ' ', text)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_tokenizer = ja_tokenizer()\n",
    "\n",
    "def wakati_reading(text:str):\n",
    "    tokens = j_tokenizer.tokenize(text.replace(\"'\", \"\").lower())\n",
    "    \n",
    "    exclude_pos = [u'助動詞']\n",
    "    \n",
    "    #分かち書き\n",
    "    tokens_w_space = \"\"\n",
    "    for token in tokens:\n",
    "        partOfSpeech = token.part_of_speech.split(',')[0]\n",
    "        \n",
    "        if partOfSpeech not in exclude_pos:\n",
    "            tokens_w_space = tokens_w_space + \" \" + token.surface\n",
    "\n",
    "    tokens_w_space = tokens_w_space.strip()\n",
    "    tokens_w_space = re.sub(r'\\s{2,}', ' ', tokens_w_space)\n",
    "    \n",
    "    return tokens_w_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text:str):\n",
    "    MAX_SEQUENCE_LENGTH = 500\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    text = wakati_reading(text)\n",
    "\n",
    "    x = tokenizer.texts_to_sequences([text])\n",
    "    x = pad_sequences(x, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_book(book_list:list):\n",
    "    book_vec = model.predict(book_list)\n",
    "    book_w2v_result = b2v.similar_by_vector(book_vec[0], topn=3)\n",
    "                  \n",
    "    similar_book_id_1 = book_w2v_result[0][0]\n",
    "    similar_book_title_1 = text_pd[text_pd.id == int(similar_book_id_1)]['title'].values[0]\n",
    "    similar_book_desc_1 = text_pd[text_pd.id == int(similar_book_id_1)]['description_token'].values[0]\n",
    "    similar_book_id_2 = book_w2v_result[1][0]\n",
    "    similar_book_title_2 = text_pd[text_pd.id == int(similar_book_id_2)]['title'].values[0]\n",
    "    similar_book_desc_2 = text_pd[text_pd.id == int(similar_book_id_2)]['description_token'].values[0]\n",
    "    similar_book_id_3 = book_w2v_result[2][0]\n",
    "    similar_book_title_3 = text_pd[text_pd.id == int(similar_book_id_3)]['title'].values[0]\n",
    "    similar_book_desc_3 = text_pd[text_pd.id == int(similar_book_id_3)]['description_token'].values[0]\n",
    "\n",
    "    print(\"1st similar book ID: {}\".format(similar_book_id_1))\n",
    "    print(\"1st similar book title and description:{}: {}\".format(similar_book_title_1, similar_book_desc_1))\n",
    "    print(\"\\n\")\n",
    "    print(\"2nd similar book ID: {}\".format(similar_book_id_2))\n",
    "    print(\"2nd similar book title and description:{}: {}\".format(similar_book_title_2, similar_book_desc_2))\n",
    "    print(\"\\n\")\n",
    "    print(\"3rd similar book ID: {}\".format(similar_book_id_3))\n",
    "    print(\"3rd similar book title and description:{}: {}\".format(similar_book_title_3, similar_book_desc_3))\n",
    "    print(\"\\n\")\n",
    "    print(\"word2vec result: {}\".format(book_w2v_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st similar book ID: 3900691\n",
      "1st similar book title and description:おじいちゃんのごくらくごくらく: おじいちゃん は お 風呂 に 入る と いつも 言い ごく らくご くらく この 言葉 は いつしか おじいちゃん と ゆ うた を つなぐ 合言葉 と なり 著者 略歴 著者 紹介 情報 より 西本 鶏 介 奈良 県 生まれ 昭和女子大学 名誉 教授 児童 文学 評論 創作 民話 研究 と 幅広く 活躍 本 データ は この 書籍 が 刊行 さ れ 当時 に 掲載 さ れ て い もの\n",
      "\n",
      "\n",
      "2nd similar book ID: 1902640\n",
      "2nd similar book title and description:菜緒のふしぎ物語: 夜 に 大 また で 歩き まわる や しき ぼう 桜 の で 舞い 踊る お ひな 様 たち 少女 と 古い 屋敷 に 住む ふしぎ もの たち と の 出会い 著者 略歴 著者 紹介 情報 より 竹内 もと 代 石川 県 生まれ 奈良 県 橿原 市 在住 近畿大学 農学部 卒業 思議 の 風 ふく 島 小峰 書店 で 第 26 回 日本 児童 文芸 家 協会 賞 第 49 回 産経 児童 出版 文化 賞 フジテレビ 賞 を 受賞 日本 児童 文芸 家 協会 日本 児童 文学 者 協会 会員 プレアデス 同人 こみ ね ゆら 熊本 市 生まれ 東京 芸術 大学 油 画 同 大学院 修 フランス 政府 給費 留学生 として 渡 仏 後 8 年 半 滞在 さくら この たんじょう 日 宮川 ひろ 文 童心 社 で 2005 年 日本 絵本 賞 を 受賞 フランス で も コーザ ベレリ 文 を 出版 本 データ は この 書籍 が 刊行 さ れ 当時 に 掲載 さ れ て い もの\n",
      "\n",
      "\n",
      "3rd similar book ID: 1902338\n",
      "3rd similar book title and description:森のおくの小さな物語: 森 に 暮らす 動物 たち の 心 あたたまる 家族 の す がた を 描い 11 の 物語 たとえば コウモリ の 歌 森 の 洞くつ で 暮らす コウモリ たち が 度 も っ こと が ない の は わけ が あり みんな 歌 が 好き から ありがとう の 歌 ごめんなさい の 歌 うち 響きわたっ て いる の たとえば お とむらい 巣 から 落ち て 死ん で しまっ コサメビタキ の 子 を とむらう 歌 を 途 で 歌え なっ おかあさん を の 風 が 草 が 助け て くれる 歌声 の ある 家 思いやり が ある 家 森 に 暮らす 動物 の 親子 が 家族 の 原点 を 教え て くれ はらだた け ひで 氏 の やさしく 温か トーン の 挿し絵 が 満載 心 を なごま せ て くれ プレゼント に も ぴったり の 1 冊 歌 ご え の ある 家 笑い が ある 家 森 に くらす 動物 の 親子 が 家族 の 原点 を 教え て くれる\n",
      "\n",
      "\n",
      "word2vec result: [('3900691', 0.6244912147521973), ('1902640', 0.6179466843605042), ('1902338', 0.61081862449646)]\n"
     ]
    }
   ],
   "source": [
    "# 名探偵コナン (1)\n",
    "book1 = \"\"\"\n",
    "▼第1話/平成のホームズ\n",
    "▼第2話/小さくなった名探偵\n",
    "▼第3話/仲間はずれの名探偵\n",
    "▼第4話/6本目の煙突\n",
    "▼第5話/もう一人の犯人\n",
    "▼第6話/迷探偵を名探偵に\n",
    "▼第7話/血ぬられたアイドル\n",
    "▼第8話/あなたに似た人\n",
    "▼第9話/不幸な誤解\n",
    "\"\"\"\n",
    "\n",
    "# おしりたんてい かいとうと ねらわれた はなよめ (おしりたんていファイル 8)\n",
    "book2 = \"\"\"\n",
    "はたして 今回の かいとうUの ねらいは…!?\n",
    "\n",
    "おしりたんていの事務所に、とつぜんあらわれた\n",
    "なぞのいらいにん。だが、そのしょうたいをたちどころに\n",
    "見抜いたわれらがおしりたんてい。なぞの依頼人が\n",
    "持ち込んだのは、なんとかいとうUからの予告状だった!\n",
    "\n",
    "いにしえからの、けっこんの儀式にかくされたお宝を\n",
    "かいとうUからまもるため、今回もおしりたんていの\n",
    "名推理が冴えわたる。\n",
    "\n",
    "迷路や、絵探しなど、\n",
    "おしりたんていといっしょに謎を解きながら、\n",
    "真実にせまる、本格的推理読み物シリーズです。\n",
    "\n",
    "今回も、「かいとうと ねらわれた はなよめ」\n",
    "「おりの なかの けいかく」の2話収録。\n",
    "\n",
    "何度読んでも発見がある\n",
    "推理小説の入り口にも最適な\n",
    "知的好奇心をくすぐる1冊です。\n",
    "\"\"\"\n",
    "\n",
    "# がっこうのおばけずかん (どうわがいっぱい)\n",
    "book3 = \"\"\"\n",
    "怖いけどおもしろい、「図鑑」という名の童話、「おばけずかん」シリーズ最新刊。「ひょうほんがいこつ」「おんがくしつのベートーベン」「トイレのはなこさん」などなど、毎日通っている学校にもこわ~いおばけはいっぱいいるけど、このお話を読めば、だいじょうぶ!\n",
    "\n",
    "\n",
    "昔から知られているおばけがいっぱい登場する、「図鑑」という名前の童話「おばけずかん」シリーズの新刊です。\n",
    "それぞれのおばけが、どんなふうに怖いのか。そうならないためには、どうしたらだいじょうぶなのかを、ユーモラスな短いお話仕立てで紹介しています。\n",
    "登場するおばけはちょっと怖いけど、ちゃんと対応してあげると、意外になさけなくて、かわいいところもあったりします。\n",
    "怖くて、笑えて、最後はホッとできる。「こわいけど、おもしろい」、新しいおばけの童話シリーズ第4弾です。\n",
    "読者に身近な小学校を舞台に、『トイレのはなこさん』のような新しいおばけや、オリジナルのおばけ『れんぞくこうちょうせんせい』も登場。シリーズの新しい展開を見せる一冊です。\n",
    "\n",
    "●この本に登場するおばけ\n",
    "\n",
    "ひょうほんがいこつ\n",
    "おんがくしつの ベートーベン\n",
    "トイレの はなこさん\n",
    "こうていの にのみやきんじろう\n",
    "ゆうれいアナウンサー\n",
    "れんぞくこうちょうせんせい\n",
    "みつめの 六ねんせい\n",
    "まよなかの まぼろしうんどうかい\n",
    "\n",
    "※漢字は使用しません\n",
    "\"\"\"\n",
    "\n",
    "# 星のカービィ 虹の島々を救え!の巻\n",
    "book4 = \"\"\"\n",
    "カービィは、リック&カイン&クーと虹の島々を救う大冒険に出発!\n",
    "\n",
    "カービィの友だち、リック&カイン&クーがやってきた!\n",
    "三人が住む虹の島々に雨が降らなくなり、困っているらしい。\n",
    "カービィはメタナイトやデデデ大王と虹の島々へ!!\n",
    "原因を知る女の子・ピリカと出会い\n",
    "雲の上に向かうと、そこにいたのは\n",
    "カービィとも仲良しな友だちの、グーイだった。\n",
    "なぜか、グーイはカービィたちに襲いかかってきて!?\n",
    "いったい、何が起こったのか……?\n",
    "事件の解決に、カービィがいどむ!!\n",
    "\"\"\"\n",
    "book1 = preprocess_text(book1)\n",
    "book2 = preprocess_text(book2)\n",
    "book3 = preprocess_text(book3)\n",
    "book4 = preprocess_text(book4)\n",
    "\n",
    "recommend_book([book1, book2, book3, book4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
